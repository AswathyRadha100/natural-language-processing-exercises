{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805dca28",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: lightblue; padding: 60px;\">\n",
    "    <h1><b>Modeling\n",
    "</b></h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e437dbb",
   "metadata": {},
   "source": [
    "In this lesson, we'll do a bit of feature engineering, and then model our text data. We'll be aiming to predict whether a given text message is spam or not, and trying to predict the category of news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5cc1fd",
   "metadata": {},
   "source": [
    "Feature Extraction: TF-IDF\n",
    " - TF: Term Frequency; how often a word appears in a document.\n",
    " - IDF: Inverse Documnet Frequency; a measure based on in how many documents will a word appear.\n",
    " - TF-IDF: A combination of the two measures above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d2680",
   "metadata": {},
   "source": [
    "Term Frequency (TF)\n",
    " - Term frequency can be calculated in a number of ways, all of which reflect how frequently a word appears in a document.\n",
    "\n",
    " - Raw Count: This is simply the count of the number of occurances of each word.\n",
    "Frequency: The number of times each word appears divided by the total number of words.\n",
    "\n",
    " - Augmented Frequency: The frequency of each word divided by the maximum frequency. This can help prevent bias towards larger documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a76ab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from prepare import basic_clean, lemmatize\n",
    "\n",
    "from math import log\n",
    "\n",
    "from env import user, password, host\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f703e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = 'Mary had a little lamb, a little lamb, a little lamb.'\n",
    "\n",
    "# clean up the text\n",
    "document = document.lower().replace(',', '').replace('.', '')\n",
    "# transform into a series\n",
    "words = pd.Series(document.split())\n",
    "\n",
    "# From the Series we can extract the value_counts, which is our raw count\n",
    "# for term frequency. Once we have the raw counts, we can calculate the\n",
    "# other measures.\n",
    "(pd.DataFrame({'raw_count': words.value_counts()})\n",
    " .assign(frequency=lambda df: df.raw_count / df.raw_count.sum())\n",
    " .assign(augmented_frequency=lambda df: df.frequency / df.frequency.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47baa6d8",
   "metadata": {},
   "source": [
    "!!!tip \"TF inputs\" The calculation for an individual TF score requires a word and a body of text (a document)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251c8626",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab37a2",
   "metadata": {},
   "source": [
    "Inverse Document Frequency also provides information about individual words, but, in order to use this measure, we must have multiple documents, i.e. several different bodies of text.\n",
    "\n",
    "Inverse Document Frequency tells us how much information a word provides. It is based on how commonly a word appears across multiple documents. The metric is divised such that the more frequently a word appears, the lower the IDF for that word will be.\n",
    "\n",
    " \n",
    "!!!note \"idf calculation\" If a given word doesn't appear in any documents, the denominator in the equation above would be zero, so some definitions of idf will add 1 to the denominator.\n",
    "\n",
    "For example, imagine we have 20 documents. We can visualize what the idf score looks like with the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b7ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_documents = 20\n",
    "\n",
    "x = np.arange(1, n_documents + 1)\n",
    "y = np.log(n_documents / x)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(x, y, marker='.')\n",
    "\n",
    "plt.xticks(x)\n",
    "plt.xlabel('# of Documents the word appears in')\n",
    "plt.ylabel('IDF')\n",
    "plt.title('IDF for a given word')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f964f",
   "metadata": {},
   "source": [
    "Now let's walk through an example of calculating IDF for multiple words. We'll use a small example dataset.\n",
    "\n",
    "First we'll prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6df7f521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': \"Codeup's data science program was created in response to a \"\n",
      "            'percieved lack of data science talent, and growing demand.',\n",
      " 'description': \"Codeup's data science program teaches hands on skills using \"\n",
      "                'Python and pandas.',\n",
      " 'news': 'Codeup announced last thursday that they just launched a new data '\n",
      "         'science program. It is 18 weeks long.'}\n",
      "\n",
      "Cleaning and lemmatizing...\n",
      "\n",
      "{'context': \"codeup's data science program wa created in response to a \"\n",
      "            'percieved lack of data science talent and growing demand',\n",
      " 'description': \"codeup's data science program teach hand on skill using \"\n",
      "                'python and panda',\n",
      " 'news': 'codeup announced last thursday that they just launched a new data '\n",
      "         'science program it is 18 week long'}\n"
     ]
    }
   ],
   "source": [
    "# our 3 example documents\n",
    "documents = {\n",
    "    'news': 'Codeup announced last thursday that they just launched a new data science program. It is 18 weeks long.',\n",
    "    'description': 'Codeup\\'s data science program teaches hands on skills using Python and pandas.',\n",
    "    'context': 'Codeup\\'s data science program was created in response to a percieved lack of data science talent, and growing demand.'\n",
    "}\n",
    "pprint(documents)\n",
    "\n",
    "print('\\nCleaning and lemmatizing...\\n')\n",
    "\n",
    "documents = {topic: lemmatize(basic_clean(documents[topic])) for topic in documents}\n",
    "pprint(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e47f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "Then we can calculate the inverse document frequency metric for each word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49fdd1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>teach</th>\n",
       "      <td>1.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created</th>\n",
       "      <td>1.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hand</th>\n",
       "      <td>1.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skill</th>\n",
       "      <td>1.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>using</th>\n",
       "      <td>1.098612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              idf\n",
       "word             \n",
       "teach    1.098612\n",
       "created  1.098612\n",
       "hand     1.098612\n",
       "skill    1.098612\n",
       "using    1.098612"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple way to calculate idf for demonstration. Note that this\n",
    "# function relies on the globally defined documents variable.\n",
    "def idf(word):\n",
    "    n_occurences = sum([1 for doc in documents.values() if word in doc])\n",
    "    return log(len(documents) / n_occurences)\n",
    "\n",
    "# Get a list of the unique words\n",
    "unique_words = pd.Series(' '.join(documents.values()).split()).unique()\n",
    "\n",
    "# put the unique words into a data frame\n",
    "(pd.DataFrame(dict(word=unique_words))\n",
    " # calculate the idf for each word\n",
    " .assign(idf=lambda df: df.word.apply(idf))\n",
    " # sort the data for presentation purposes\n",
    " .set_index('word')\n",
    " .sort_values(by='idf', ascending=False)\n",
    " .head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ab2201",
   "metadata": {},
   "source": [
    "A higher IDF means that a word provides more information. That is, it is more relevant within a single document.\n",
    "\n",
    "!!!tip \"IDF inputs\" The calculation for an individual IDF score requires a word and a set of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47bf3be",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3bc2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF-IDF is simply the multiplication of the two metrics we've discussed above. Let's calculate an TF-IDF for all of the words and documents:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e0fcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs = []\n",
    "\n",
    "# We'll caclulate the tf-idf value for every word across every document\n",
    "\n",
    "# Start by iterating over all the documents\n",
    "for doc, text in documents.items():\n",
    "    # We'll make a data frame that contains the tf for every word in every document\n",
    "    df = (pd.Series(text.split())\n",
    "          .value_counts()\n",
    "          .reset_index()\n",
    "          .set_axis(['word', 'raw_count'], axis=1)\n",
    "          .assign(tf=lambda df: df.raw_count / df.shape[0])\n",
    "          .drop(columns='raw_count')\n",
    "          .assign(doc=doc))\n",
    "    # Then add that data frame to our list\n",
    "    tfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e355b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tf</th>\n",
       "      <th>doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>science</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>codeup's</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>percieved</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>growing</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>talent</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>of</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lack</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>to</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>response</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>in</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>created</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>wa</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>program</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>demand</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word        tf      doc\n",
       "0     science  0.117647  context\n",
       "1        data  0.117647  context\n",
       "2    codeup's  0.058824  context\n",
       "3   percieved  0.058824  context\n",
       "4     growing  0.058824  context\n",
       "5         and  0.058824  context\n",
       "6      talent  0.058824  context\n",
       "7          of  0.058824  context\n",
       "8        lack  0.058824  context\n",
       "9          to  0.058824  context\n",
       "10          a  0.058824  context\n",
       "11   response  0.058824  context\n",
       "12         in  0.058824  context\n",
       "13    created  0.058824  context\n",
       "14         wa  0.058824  context\n",
       "15    program  0.058824  context\n",
       "16     demand  0.058824  context"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "270b202e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>doc</th>\n",
       "      <th>tf_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hand</td>\n",
       "      <td>description</td>\n",
       "      <td>0.091551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>teach</td>\n",
       "      <td>description</td>\n",
       "      <td>0.091551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>panda</td>\n",
       "      <td>description</td>\n",
       "      <td>0.091551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>python</td>\n",
       "      <td>description</td>\n",
       "      <td>0.091551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>using</td>\n",
       "      <td>description</td>\n",
       "      <td>0.091551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>skill</td>\n",
       "      <td>description</td>\n",
       "      <td>0.091551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>wa</td>\n",
       "      <td>context</td>\n",
       "      <td>0.064624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>created</td>\n",
       "      <td>context</td>\n",
       "      <td>0.064624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>response</td>\n",
       "      <td>context</td>\n",
       "      <td>0.064624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>to</td>\n",
       "      <td>context</td>\n",
       "      <td>0.064624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lack</td>\n",
       "      <td>context</td>\n",
       "      <td>0.064624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>of</td>\n",
       "      <td>context</td>\n",
       "      <td>0.064624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>talent</td>\n",
       "      <td>context</td>\n",
       "      <td>0.064624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>growing</td>\n",
       "      <td>context</td>\n",
       "      <td>0.064624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>percieved</td>\n",
       "      <td>context</td>\n",
       "      <td>0.064624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>demand</td>\n",
       "      <td>context</td>\n",
       "      <td>0.064624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>last</td>\n",
       "      <td>news</td>\n",
       "      <td>0.061034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>that</td>\n",
       "      <td>news</td>\n",
       "      <td>0.061034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>new</td>\n",
       "      <td>news</td>\n",
       "      <td>0.061034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>long</td>\n",
       "      <td>news</td>\n",
       "      <td>0.061034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>news</td>\n",
       "      <td>0.061034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>launched</td>\n",
       "      <td>news</td>\n",
       "      <td>0.061034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>just</td>\n",
       "      <td>news</td>\n",
       "      <td>0.061034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>they</td>\n",
       "      <td>news</td>\n",
       "      <td>0.061034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>announced</td>\n",
       "      <td>news</td>\n",
       "      <td>0.061034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>thursday</td>\n",
       "      <td>news</td>\n",
       "      <td>0.061034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>news</td>\n",
       "      <td>0.061034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>week</td>\n",
       "      <td>news</td>\n",
       "      <td>0.061034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>it</td>\n",
       "      <td>news</td>\n",
       "      <td>0.061034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>and</td>\n",
       "      <td>description</td>\n",
       "      <td>0.033789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>codeup's</td>\n",
       "      <td>description</td>\n",
       "      <td>0.033789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>in</td>\n",
       "      <td>context</td>\n",
       "      <td>0.023851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>codeup's</td>\n",
       "      <td>context</td>\n",
       "      <td>0.023851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>context</td>\n",
       "      <td>0.023851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>program</td>\n",
       "      <td>news</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>program</td>\n",
       "      <td>context</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a</td>\n",
       "      <td>context</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data</td>\n",
       "      <td>context</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>science</td>\n",
       "      <td>news</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>data</td>\n",
       "      <td>news</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a</td>\n",
       "      <td>news</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>science</td>\n",
       "      <td>context</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>on</td>\n",
       "      <td>description</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>program</td>\n",
       "      <td>description</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>science</td>\n",
       "      <td>description</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data</td>\n",
       "      <td>description</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>codeup</td>\n",
       "      <td>news</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word          doc    tf_idf\n",
       "5        hand  description  0.091551\n",
       "4       teach  description  0.091551\n",
       "11      panda  description  0.091551\n",
       "9      python  description  0.091551\n",
       "8       using  description  0.091551\n",
       "7       skill  description  0.091551\n",
       "14         wa      context  0.064624\n",
       "13    created      context  0.064624\n",
       "11   response      context  0.064624\n",
       "9          to      context  0.064624\n",
       "8        lack      context  0.064624\n",
       "7          of      context  0.064624\n",
       "6      talent      context  0.064624\n",
       "4     growing      context  0.064624\n",
       "3   percieved      context  0.064624\n",
       "16     demand      context  0.064624\n",
       "16       last         news  0.061034\n",
       "14       that         news  0.061034\n",
       "9         new         news  0.061034\n",
       "17       long         news  0.061034\n",
       "4          is         news  0.061034\n",
       "11   launched         news  0.061034\n",
       "12       just         news  0.061034\n",
       "13       they         news  0.061034\n",
       "1   announced         news  0.061034\n",
       "15   thursday         news  0.061034\n",
       "3          18         news  0.061034\n",
       "2        week         news  0.061034\n",
       "5          it         news  0.061034\n",
       "10        and  description  0.033789\n",
       "0    codeup's  description  0.033789\n",
       "12         in      context  0.023851\n",
       "2    codeup's      context  0.023851\n",
       "5         and      context  0.023851\n",
       "6     program         news  0.000000\n",
       "15    program      context  0.000000\n",
       "10          a      context  0.000000\n",
       "1        data      context  0.000000\n",
       "7     science         news  0.000000\n",
       "8        data         news  0.000000\n",
       "10          a         news  0.000000\n",
       "0     science      context  0.000000\n",
       "6          on  description  0.000000\n",
       "3     program  description  0.000000\n",
       "2     science  description  0.000000\n",
       "1        data  description  0.000000\n",
       "0      codeup         news  0.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll then concatenate all the tf values together.\n",
    "(pd.concat(tfs)\n",
    " # calculate the idf value for each word\n",
    " .assign(idf=lambda df: df.word.apply(idf))\n",
    " # then use the if and idf values to calculate tf-idf \n",
    " .assign(tf_idf=lambda df: df.idf * df.tf)\n",
    " .drop(columns=['tf', 'idf'])\n",
    " .sort_values(by='tf_idf', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4db9cc4",
   "metadata": {},
   "source": [
    "It's more common to see the data presented with the words as features, and the documents as observations, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "addc6e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>word</th>\n",
       "      <th>18</th>\n",
       "      <th>a</th>\n",
       "      <th>and</th>\n",
       "      <th>announced</th>\n",
       "      <th>codeup</th>\n",
       "      <th>codeup's</th>\n",
       "      <th>created</th>\n",
       "      <th>data</th>\n",
       "      <th>demand</th>\n",
       "      <th>growing</th>\n",
       "      <th>...</th>\n",
       "      <th>skill</th>\n",
       "      <th>talent</th>\n",
       "      <th>teach</th>\n",
       "      <th>that</th>\n",
       "      <th>they</th>\n",
       "      <th>thursday</th>\n",
       "      <th>to</th>\n",
       "      <th>using</th>\n",
       "      <th>wa</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>context</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023851</td>\n",
       "      <td>0.064624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064624</td>\n",
       "      <td>0.064624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064624</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>description</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.061034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061034</td>\n",
       "      <td>0.061034</td>\n",
       "      <td>0.061034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "word               18    a       and  announced  codeup  codeup's   created  \\\n",
       "doc                                                                           \n",
       "context      0.000000  0.0  0.023851   0.000000     0.0  0.023851  0.064624   \n",
       "description  0.000000  0.0  0.033789   0.000000     0.0  0.033789  0.000000   \n",
       "news         0.061034  0.0  0.000000   0.061034     0.0  0.000000  0.000000   \n",
       "\n",
       "word         data    demand   growing  ...     skill    talent     teach  \\\n",
       "doc                                    ...                                 \n",
       "context       0.0  0.064624  0.064624  ...  0.000000  0.064624  0.000000   \n",
       "description   0.0  0.000000  0.000000  ...  0.091551  0.000000  0.091551   \n",
       "news          0.0  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "word             that      they  thursday        to     using        wa  \\\n",
       "doc                                                                       \n",
       "context      0.000000  0.000000  0.000000  0.064624  0.000000  0.064624   \n",
       "description  0.000000  0.000000  0.000000  0.000000  0.091551  0.000000   \n",
       "news         0.061034  0.061034  0.061034  0.000000  0.000000  0.000000   \n",
       "\n",
       "word             week  \n",
       "doc                    \n",
       "context      0.000000  \n",
       "description  0.000000  \n",
       "news         0.061034  \n",
       "\n",
       "[3 rows x 38 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll then concatenate all the tf values together.\n",
    "(pd.concat(tfs)\n",
    " # calculate the idf value for each word\n",
    " .assign(idf=lambda df: df.word.apply(idf))\n",
    " # then use the if and idf values to calculate tf-idf \n",
    " .assign(tf_idf=lambda df: df.idf * df.tf)\n",
    " .drop(columns=['tf', 'idf'])\n",
    " .sort_values(by='tf_idf', ascending=False)\n",
    " .pipe(lambda df: pd.crosstab(df.doc, df.word, values=df.tf_idf, aggfunc=lambda x: x))\n",
    " .fillna(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d8c42",
   "metadata": {},
   "source": [
    "## TF-IDF with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33857ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x36 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 45 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidfs = tfidf.fit_transform(documents.values())\n",
    "tfidfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83ba45",
   "metadata": {},
   "source": [
    "We get back a sparse matrix, a matrix with more 0s than anything else. Numpy has a special type that makes some manipulations and operations faster on sparse matrices.\n",
    "\n",
    "Becuase our data set is pretty small, we can convert our sparse matrix to a regular one, and put everything in a dataframe. If our data were larger, the operation below might take much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aa87c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>18</th>\n",
       "      <th>and</th>\n",
       "      <th>announced</th>\n",
       "      <th>codeup</th>\n",
       "      <th>created</th>\n",
       "      <th>data</th>\n",
       "      <th>demand</th>\n",
       "      <th>growing</th>\n",
       "      <th>hand</th>\n",
       "      <th>in</th>\n",
       "      <th>...</th>\n",
       "      <th>skill</th>\n",
       "      <th>talent</th>\n",
       "      <th>teach</th>\n",
       "      <th>that</th>\n",
       "      <th>they</th>\n",
       "      <th>thursday</th>\n",
       "      <th>to</th>\n",
       "      <th>using</th>\n",
       "      <th>wa</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.263566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263566</td>\n",
       "      <td>0.155666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263566</td>\n",
       "      <td>0.263566</td>\n",
       "      <td>0.263566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152159</td>\n",
       "      <td>0.257627</td>\n",
       "      <td>0.304317</td>\n",
       "      <td>0.257627</td>\n",
       "      <td>0.257627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257627</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         18       and  announced    codeup   created      data    demand  \\\n",
       "0  0.263566  0.000000   0.263566  0.155666  0.000000  0.155666  0.000000   \n",
       "1  0.000000  0.253880   0.000000  0.197160  0.000000  0.197160  0.000000   \n",
       "2  0.000000  0.195932   0.000000  0.152159  0.257627  0.304317  0.257627   \n",
       "\n",
       "    growing      hand        in  ...     skill    talent     teach      that  \\\n",
       "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.263566   \n",
       "1  0.000000  0.333821  0.000000  ...  0.333821  0.000000  0.333821  0.000000   \n",
       "2  0.257627  0.000000  0.257627  ...  0.000000  0.257627  0.000000  0.000000   \n",
       "\n",
       "       they  thursday        to     using        wa      week  \n",
       "0  0.263566  0.263566  0.000000  0.000000  0.000000  0.263566  \n",
       "1  0.000000  0.000000  0.000000  0.333821  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.257627  0.000000  0.257627  0.000000  \n",
       "\n",
       "[3 rows x 36 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tfidfs.todense(), columns=tfidf.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726c7b59",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71181a",
   "metadata": {},
   "source": [
    "Now we'll use the computed TF-IDF values as features in a model. We'll take a look at the spam data set first.\n",
    "\n",
    "Because of the way we are modeling the data, we have a lot of columns, and it is not uncommon to have more columns than rows. Also, our data is very imbalanced in the class distribution, that is, there are many more ham messages than spam messages.\n",
    "\n",
    "Other than these considerations, we can treat this as a standard classification problem. We'll use logistic regression as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f34bec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "id                                                         \n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_db_url(database, host=host, user=user, password=password):\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{database}'\n",
    "\n",
    "url = get_db_url(\"spam_db\")\n",
    "sql = \"SELECT * FROM spam\"\n",
    "\n",
    "df = pd.read_sql(sql, url, index_col=\"id\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12448957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text: str) -> list:\n",
    "    'A simple function to cleanup text data'\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    text = (text.encode('ascii', 'ignore')\n",
    "             .decode('utf-8', 'ignore')\n",
    "             .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split() # tokenization\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e874566",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df.text.apply(clean).apply(' '.join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "852163c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.clean_text\n",
    "y = df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f230a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68501c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)\n",
    "\n",
    "train = pd.DataFrame(dict(actual=y_train))\n",
    "test = pd.DataFrame(dict(actual=y_test))\n",
    "\n",
    "lm = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "train['predicted'] = lm.predict(X_train)\n",
    "test['predicted'] = lm.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0ca85c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.52%\n",
      "---\n",
      "Confusion Matrix\n",
      "actual      ham  spam\n",
      "predicted            \n",
      "ham        3852   148\n",
      "spam          7   450\n",
      "---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      1.00      0.98      3859\n",
      "        spam       0.98      0.75      0.85       598\n",
      "\n",
      "    accuracy                           0.97      4457\n",
      "   macro avg       0.97      0.88      0.92      4457\n",
      "weighted avg       0.97      0.97      0.96      4457\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.2%}'.format(accuracy_score(train.actual, train.predicted)))\n",
    "print('---')\n",
    "print('Confusion Matrix')\n",
    "print(pd.crosstab(train.predicted, train.actual))\n",
    "print('---')\n",
    "print(classification_report(train.actual, train.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba971804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.96%\n",
      "---\n",
      "Confusion Matrix\n",
      "actual     ham  spam\n",
      "predicted           \n",
      "ham        966    45\n",
      "spam         0   104\n",
      "---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      1.00      0.98       966\n",
      "        spam       1.00      0.70      0.82       149\n",
      "\n",
      "    accuracy                           0.96      1115\n",
      "   macro avg       0.98      0.85      0.90      1115\n",
      "weighted avg       0.96      0.96      0.96      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.2%}'.format(accuracy_score(test.actual, test.predicted)))\n",
    "print('---')\n",
    "print('Confusion Matrix')\n",
    "print(pd.crosstab(test.predicted, test.actual))\n",
    "print('---')\n",
    "print(classification_report(test.actual, test.predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683f01c4",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65e000c",
   "metadata": {},
   "source": [
    " - Take the work we did in the lessons further:\n",
    "\n",
    "    - What other types of models (i.e. different classifcation algorithms) could you use?\n",
    "    - How do the models compare when trained on term frequency data alone, instead of TF-IDF values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b5b5f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (TF-IDF) Accuracy (TF-IDF): 95.78%\n",
      "Multinomial Naive Bayes (TF-IDF) Accuracy (TF-IDF): 95.96%\n",
      "SVM (TF-IDF) Accuracy (TF-IDF): 98.03%\n",
      "Random Forest (TF-IDF) Accuracy (TF-IDF): 97.49%\n",
      "Logistic Regression (TF-IDF) Accuracy (TF): 98.12%\n",
      "Multinomial Naive Bayes (TF-IDF) Accuracy (TF): 98.30%\n",
      "SVM (TF-IDF) Accuracy (TF): 98.21%\n",
      "Random Forest (TF-IDF) Accuracy (TF): 97.76%\n",
      "Classification Report for Logistic Regression (TF-IDF) (TF-IDF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       966\n",
      "           1       0.96      0.71      0.82       149\n",
      "\n",
      "    accuracy                           0.96      1115\n",
      "   macro avg       0.96      0.85      0.90      1115\n",
      "weighted avg       0.96      0.96      0.96      1115\n",
      "\n",
      "Classification Report for Logistic Regression (TF-IDF) (TF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       966\n",
      "           1       0.99      0.87      0.92       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.93      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n",
      "Classification Report for Multinomial Naive Bayes (TF-IDF) (TF-IDF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       966\n",
      "           1       1.00      0.70      0.82       149\n",
      "\n",
      "    accuracy                           0.96      1115\n",
      "   macro avg       0.98      0.85      0.90      1115\n",
      "weighted avg       0.96      0.96      0.96      1115\n",
      "\n",
      "Classification Report for Multinomial Naive Bayes (TF-IDF) (TF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       966\n",
      "           1       0.93      0.94      0.94       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.96      0.96      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n",
      "Classification Report for SVM (TF-IDF) (TF-IDF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       966\n",
      "           1       0.97      0.88      0.92       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.94      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n",
      "Classification Report for SVM (TF-IDF) (TF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       966\n",
      "           1       0.99      0.87      0.93       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.94      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n",
      "Classification Report for Random Forest (TF-IDF) (TF-IDF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99       966\n",
      "           1       1.00      0.81      0.90       149\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.99      0.91      0.94      1115\n",
      "weighted avg       0.98      0.97      0.97      1115\n",
      "\n",
      "Classification Report for Random Forest (TF-IDF) (TF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99       966\n",
      "           1       1.00      0.83      0.91       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.92      0.95      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Modeling\n",
    "\n",
    "# Set up your functions for text cleaning\n",
    "def clean(text):\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    text = (text.encode('ascii', 'ignore')\n",
    "             .decode('utf-8', 'ignore')\n",
    "             .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load your dataset and preprocess the text\n",
    "url = get_db_url(\"spam_db\")\n",
    "sql = \"SELECT * FROM spam\"\n",
    "df = pd.read_sql(sql, url, index_col=\"id\")\n",
    "df['clean_text'] = df.text.apply(clean).apply(' '.join)\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "X = df.clean_text\n",
    "y = df.label\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\n",
    "\n",
    "# Initialize different classification models\n",
    "models = {\n",
    "    'Logistic Regression (TF-IDF)': LogisticRegression(),\n",
    "    'Multinomial Naive Bayes (TF-IDF)': MultinomialNB(),\n",
    "    'SVM (TF-IDF)': SVC(),\n",
    "    'Random Forest (TF-IDF)': RandomForestClassifier(),\n",
    "}\n",
    "\n",
    "# Encode target classes\n",
    "y_train_encoded = y_train.map({'ham': 0, 'spam': 1})\n",
    "y_test_encoded = y_test.map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# TF Vectorization\n",
    "tf_vectorizer = CountVectorizer()\n",
    "X_train_tf = tf_vectorizer.fit_transform(X_train)\n",
    "X_test_tf = tf_vectorizer.transform(X_test)\n",
    "\n",
    "# Evaluate models with TF-IDF features\n",
    "for model_name, model in models.items():\n",
    "    lm = model.fit(X_train_tfidf, y_train_encoded)\n",
    "    test['predicted_' + model_name] = lm.predict(X_test_tfidf)\n",
    "    accuracy = accuracy_score(y_test_encoded, test['predicted_' + model_name])\n",
    "    print(f'{model_name} Accuracy (TF-IDF): {accuracy:.2%}')\n",
    "\n",
    "# Train models with raw TF features\n",
    "for model_name, model in models.items():\n",
    "    lm = model.fit(X_train_tf, y_train_encoded)\n",
    "    test['predicted_' + model_name + ' (TF)'] = lm.predict(X_test_tf)\n",
    "    accuracy = accuracy_score(y_test_encoded, test['predicted_' + model_name + ' (TF)'])\n",
    "    print(f'{model_name} Accuracy (TF): {accuracy:.2%}')\n",
    "\n",
    "# Print classification reports for the models\n",
    "for model_name in models:\n",
    "    print(f'Classification Report for {model_name} (TF-IDF):')\n",
    "    print(classification_report(y_test_encoded, test['predicted_' + model_name]))\n",
    "    \n",
    "    print(f'Classification Report for {model_name} (TF):')\n",
    "    print(classification_report(y_test_encoded, test['predicted_' + model_name + ' (TF)']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc93653f",
   "metadata": {},
   "source": [
    "Summary:-\n",
    "Models Trained on TF-IDF Features:\n",
    "Logistic Regression: Accuracy - 95.78%\n",
    "Multinomial Naive Bayes: Accuracy - 95.96%\n",
    "SVM: Accuracy - 98.03%\n",
    "Random Forest: Accuracy - 97.49%\n",
    "Models Trained on TF Features:\n",
    "Logistic Regression: Accuracy - 98.12%\n",
    "Multinomial Naive Bayes: Accuracy - 98.30%\n",
    "SVM: Accuracy - 98.21%\n",
    "Random Forest: Accuracy - 97.76%\n",
    "\n",
    "For both feature types (TF-IDF and TF), the models achieved high accuracy. Some models, like SVM and Naive Bayes, performed equally well or even slightly better when trained on TF features compared to TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cbc517",
   "metadata": {},
   "source": [
    "Note: TF-IDF takes into account the importance of words in the corpus, which can be valuable for certain tasks. However, TF features can be simpler and still yield excellent results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daad7b4",
   "metadata": {},
   "source": [
    "## Using XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75603a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.41%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       966\n",
      "           1       0.98      0.74      0.85       149\n",
      "\n",
      "    accuracy                           0.96      1115\n",
      "   macro avg       0.97      0.87      0.91      1115\n",
      "weighted avg       0.96      0.96      0.96      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the target variable\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Train an XGBoost classifier\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train_tfidf, y_train_encoded)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_encoded = model.predict(X_test_tfidf)\n",
    "\n",
    "# Decode the predictions back to original labels (if needed)\n",
    "y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred_encoded)\n",
    "print(f'Accuracy: {accuracy:.2%}')\n",
    "\n",
    "# Print a classification report\n",
    "print(classification_report(y_test_encoded, y_pred_encoded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fc3dd1",
   "metadata": {},
   "source": [
    "Summary:-\n",
    "Model performs well in identifying 'ham' messages, achieving high precision and recall. While the model is slightly less precise in identifying 'spam' messages, it still maintains good overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb4919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
